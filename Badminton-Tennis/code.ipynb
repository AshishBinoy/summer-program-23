{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediapipe code to extract body coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "from mediapipe.python.solutions.pose import PoseLandmark\n",
    "\n",
    "def find_coordinates(video_name, file_name):\n",
    "\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # video_file = 'FinalVid.mp4'\n",
    "    video_file = video_name\n",
    "\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    f = open(file_name,'w')\n",
    "    writer = csv.writer(f)\n",
    "    # writer.writerow(['Frame', 'Nose_X', 'Nose_Y', 'Left_Shoulder_X', 'Left_Shoulder_Y', 'Right_Shoulder_X', 'Right_Shoulder_Y', ...])  # Add all the required column names\n",
    "    writer.writerow(['Frame', 'Nose_X', 'Nose_Y', 'Left_Shoulder_X', 'Left_Shoulder_Y', 'Right_Shoulder_X', 'Right_Shoulder_Y', 'Left_Elbow_X', 'Left_Elbow_Y', 'Right_Elbow_X', 'Right_Elbow_Y', 'Left_Wrist_X', 'Left_Wrist_Y', 'Right_Wrist_X', 'Right_Wrist_Y', 'Left_Hip_X', 'Left_Hip_Y', 'Right_Hip_X', 'Right_Hip_Y', 'Left_Knee_X', 'Left_Knee_Y', 'Right_Knee_X', 'Right_Knee_Y', 'Left_Ankle_X', 'Left_Ankle_Y', 'Right_Ankle_X', 'Right_Ankle_Y'])\n",
    "\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.1, min_tracking_confidence=0.1) as pose:\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                print(\"End of video\")\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_rgb.flags.writeable = False\n",
    "            results = pose.process(image_rgb)\n",
    "            image_rgb.flags.writeable = True\n",
    "            image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # row = [frame_count, \n",
    "                #        landmarks[mp_pose.PoseLandmark.NOSE].x, landmarks[mp_pose.PoseLandmark.NOSE].y,\n",
    "                #        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y,\n",
    "                #        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y,\n",
    "                #        ...]  # Add all the required landmark coordinates in the desired order\n",
    "                row = [frame_count,\n",
    "                        landmarks[PoseLandmark.NOSE].x, landmarks[PoseLandmark.NOSE].y,\n",
    "                        landmarks[PoseLandmark.LEFT_SHOULDER].x, landmarks[PoseLandmark.LEFT_SHOULDER].y,\n",
    "                        landmarks[PoseLandmark.RIGHT_SHOULDER].x, landmarks[PoseLandmark.RIGHT_SHOULDER].y,\n",
    "                        landmarks[PoseLandmark.LEFT_ELBOW].x, landmarks[PoseLandmark.LEFT_ELBOW].y,\n",
    "                        landmarks[PoseLandmark.RIGHT_ELBOW].x, landmarks[PoseLandmark.RIGHT_ELBOW].y,\n",
    "                        landmarks[PoseLandmark.LEFT_WRIST].x, landmarks[PoseLandmark.LEFT_WRIST].y,\n",
    "                        landmarks[PoseLandmark.RIGHT_WRIST].x, landmarks[PoseLandmark.RIGHT_WRIST].y,\n",
    "                        landmarks[PoseLandmark.LEFT_HIP].x, landmarks[PoseLandmark.LEFT_HIP].y,\n",
    "                        landmarks[PoseLandmark.RIGHT_HIP].x, landmarks[PoseLandmark.RIGHT_HIP].y,\n",
    "                        landmarks[PoseLandmark.LEFT_KNEE].x, landmarks[PoseLandmark.LEFT_KNEE].y,\n",
    "                        landmarks[PoseLandmark.RIGHT_KNEE].x, landmarks[PoseLandmark.RIGHT_KNEE].y,\n",
    "                        landmarks[PoseLandmark.LEFT_ANKLE].x, landmarks[PoseLandmark.LEFT_ANKLE].y,\n",
    "                        landmarks[PoseLandmark.RIGHT_ANKLE].x, landmarks[PoseLandmark.RIGHT_ANKLE].y]\n",
    "\n",
    "                writer.writerow(row)            \n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(image_bgr, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                \n",
    "            cv2.imshow('Pose Estimation Video', image_bgr)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    f.close()\n",
    "\n",
    "# print(\"Enter the video name: \")\n",
    "# video = input()\n",
    "# print(\"Enter the CSV file name: \")\n",
    "# file_name = input()\n",
    "# find_coordinates(video, file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Deep Learning Model Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from mp_2 import find_coordinates\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def predictor(xi, yi):\n",
    "    csv_files = ['Smash1.csv', 'Smash2.csv', 'Smash3.csv', 'Smash4.csv', 'Smash5.csv']\n",
    "\n",
    "    # Load and concatenate the data from multiple CSV files\n",
    "    data = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = pd.concat([data, df], ignore_index=True)\n",
    "\n",
    "    # Create the features and labels\n",
    "    X = data[xi].values.reshape(-1, 1)  # Training variable\n",
    "    y = data[yi].values.reshape(-1, 1)  # Target variable\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert the NumPy arrays to float32\n",
    "    X_train_scaled = X_train_scaled.astype(np.float32)\n",
    "    X_test_scaled = X_test_scaled.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(1,)))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model with the Adam optimizer\n",
    "    optimizer = Adam(learning_rate=0.001)  # Adjust the learning rate as needed\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Inverse transform the scaled data to get the original values\n",
    "    y_test_original = scaler.inverse_transform(y_test)\n",
    "    y_pred_original = scaler.inverse_transform(y_pred)\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE) and R-squared (R2) score\n",
    "    mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"R-squared (R2) Score:\", r2)\n",
    "    print(\"Percentage of Variance Explained by R2: {:.2f}%\".format(r2 * 100))\n",
    "\n",
    "    # Plotting predicted coordinates\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_test, y_pred_original, color='red', label='Predicted')\n",
    "    plt.xlabel('X Coordinate Intervals')\n",
    "    plt.ylabel('Predicted Y Coordinate')\n",
    "    plt.title('Predicted Coordinates')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting actual coordinates\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_test, y_test_original, color='blue', label='Actual')\n",
    "    plt.xlabel('X Coordinate Intervals')\n",
    "    plt.ylabel('Actual Y Coordinate')\n",
    "    plt.title('Actual Coordinates')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "print(\"Enter the video name: \")\n",
    "video = input()\n",
    "print(\"Enter the CSV file name: \")\n",
    "file_name = input()\n",
    "find_coordinates(video, file_name)\n",
    "\n",
    "print(\"What do you want to train for?\\n\")\n",
    "print(\"Please Enter: \")\n",
    "inp = input()\n",
    "\n",
    "match inp:\n",
    "    case \"Right Shoulders\":\n",
    "        predictor('Right_Shoulder_X', 'Right_Shoulder_Y')\n",
    "    case \"Left Shoulders\":\n",
    "        predictor('Left_Shoulder_X', 'Right_Shoulder_Y')\n",
    "    case \"Right Elbow\":\n",
    "        predictor('Right_Elbow_X', 'Right_Elbow_Y')\n",
    "    case \"Left Elbow\":\n",
    "        predictor('Left_Elbow_X', 'Right_Elbow_Y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from mp_2 import find_coordinates\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# List of CSV files to load\n",
    "def predictor(xi, yi):\n",
    "    csv_files = ['Smash1.csv', 'Smash2.csv', 'Smash3.csv', 'Smash4.csv',\n",
    "                 'Smash5.csv', 'Smash6.csv', 'Smash7.csv', 'Smash8.csv',\n",
    "                 'Smash9.csv', 'Smash10.csv', 'Smash11.csv', 'Smash12.csv',\n",
    "                 'Smash13.csv', 'Smash14.csv', 'Smash15.csv', 'Smash16.csv']\n",
    "\n",
    "    # Load and concatenate the data from multiple CSV files\n",
    "    data = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = pd.concat([data, df], ignore_index=True)\n",
    "\n",
    "    # Create the features and labels\n",
    "    X = data[xi].values.reshape(-1, 1)  # Training variable\n",
    "    y = data[yi].values.reshape(-1, 1)  # Target variable\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize the RandomForestRegressor\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    df1 = pd.read_csv(file_name)\n",
    "    x_input = df1[xi].values.reshape(-1, 1)\n",
    "    y_input = df1[yi].values.reshape(-1, 1)\n",
    "    li_scaled = scaler.transform(x_input)\n",
    "    predicted_values = model.predict(li_scaled)\n",
    "\n",
    "    # Fit a quadratic curve to the predicted values using Polynomial Regression\n",
    "    poly_reg = PolynomialFeatures(degree=2)\n",
    "    x_input_poly = poly_reg.fit_transform(x_input)\n",
    "    line_reg = LinearRegression()\n",
    "    line_reg.fit(x_input_poly, predicted_values)\n",
    "    line_fit = line_reg.predict(x_input_poly)\n",
    "\n",
    "    # Plotting the predicted curve and actual values\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x_input, predicted_values, color='red', label='Predicted')\n",
    "    plt.plot(x_input, line_fit, color='green', label='Curve of Best Fit')\n",
    "    plt.xlabel('X Coordinate Intervals')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.title('Predicted Coordinates')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Fit a quadratic curve to the actual values using Polynomial Regression\n",
    "    actual_x_poly = poly_reg.fit_transform(x_input)\n",
    "    line_reg_actual = LinearRegression()\n",
    "    line_reg_actual.fit(actual_x_poly, y_input)\n",
    "    line_fit_actual = line_reg_actual.predict(actual_x_poly)\n",
    "\n",
    "    # Plotting the actual values\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x_input, y_input, color='blue', label='Actual')\n",
    "    plt.plot(x_input, line_fit_actual, color='green', label='Curve of Best Fit (Actual)')\n",
    "    plt.xlabel('X Coordinate Intervals')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.title('Actual Coordinates')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Inverse transform the scaled data to get the original values\n",
    "    y_test_original = scaler.inverse_transform(y_test)\n",
    "    y_pred_original = scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE) and R-squared (R2) score\n",
    "    mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "    mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"R-squared (R2) Score:\", r2)\n",
    "    print(\"Percentage of Variance Explained by R2: {:.2f}%\".format(r2 * 100))\n",
    "\n",
    "\n",
    "print(\"Enter the video name: \")\n",
    "video = input()\n",
    "print(\"Enter the CSV file name: \")\n",
    "file_name = input()\n",
    "find_coordinates(video, file_name)\n",
    "\n",
    "print(\"What do you want to train for?\\n\")\n",
    "print(\"Please Enter: \")\n",
    "inp = input()\n",
    "\n",
    "match inp:\n",
    "    case \"Right Shoulders\":\n",
    "        predictor('Right_Shoulder_X', 'Right_Shoulder_Y')\n",
    "    case \"Left Shoulders\":\n",
    "        predictor('Left_Shoulder_X', 'Right_Shoulder_Y')\n",
    "    case \"Right Elbow\":\n",
    "        predictor('Right_Elbow_X', 'Right_Elbow_Y')\n",
    "    case \"Left Elbow\":\n",
    "        predictor('Left_Elbow_X', 'Right_Elbow_Y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add player tracking using bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "\n",
    "def player_tracking(video_path, output_csv_path):\n",
    "    # used MIL tracker\n",
    "    tracker = cv2.TrackerMIL_create()\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening the video file.\")\n",
    "        return\n",
    "\n",
    "    # get first frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame.\")\n",
    "        return\n",
    "\n",
    "    # Selected player's initial bounding box\n",
    "    bbox = cv2.selectROI(\"Select Player\", frame, fromCenter=False, showCrosshair=True)\n",
    "    tracker.init(frame, bbox)\n",
    "\n",
    "    # CSV file to write player coordinates\n",
    "    with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Player_X', 'Player_Y'])\n",
    "\n",
    "        # processing loop for player tracking\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Update tracker to track the player in the current frame\n",
    "            success, bbox = tracker.update(frame)\n",
    "\n",
    "            if success:\n",
    "                # Get the center coordinates of the bounding box\n",
    "                player_x = int(bbox[0] + bbox[2] / 2)\n",
    "                player_y = int(bbox[1] + bbox[3] / 2)\n",
    "\n",
    "                # player coordinates to the CSV file\n",
    "                csv_writer.writerow([player_x, player_y])\n",
    "\n",
    "                # Draw the bounding box and player's position on the frame\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])), (0, 255, 0), 2)\n",
    "                cv2.circle(frame, (player_x, player_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "            # Display current frame with player tracking\n",
    "            cv2.imshow(\"Player Tracking\", frame)\n",
    "\n",
    "            # Press 'q' to exit \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "video_path = \"match.mp4\"\n",
    "output_csv_path = \"player_track.csv\"\n",
    "\n",
    "# Start player tracking\n",
    "player_tracking(video_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO Code to calculate the total time of the player in the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "from boxmot.tracker_zoo import create_tracker\n",
    "from boxmot.utils import ROOT, WEIGHTS\n",
    "from boxmot.utils import logger as LOGGER\n",
    "from boxmot.utils.checks import TestRequirements\n",
    "from boxmot.utils.torch_utils import select_device\n",
    "from detectors.strategy import get_yolo_inferer\n",
    "from detectors.yolo_processor import Yolo\n",
    "from ultralytics.yolo.data.utils import VID_FORMATS\n",
    "from ultralytics.yolo.engine.model import TASK_MAP, YOLO\n",
    "from ultralytics.yolo.utils import IterableSimpleNamespace, colorstr, ops\n",
    "from ultralytics.yolo.utils.checks import check_imgsz\n",
    "from ultralytics.yolo.utils.files import increment_path\n",
    "from ultralytics.yolo.utils.plotting import save_one_box\n",
    "from utils import write_MOT_results\n",
    "from boxmot.utils import EXAMPLES\n",
    "\n",
    "\n",
    "def on_predict_start(predictor):\n",
    "    predictor.trackers = []\n",
    "    predictor.tracker_outputs = [None] * predictor.dataset.bs\n",
    "    predictor.args.tracking_config = \\\n",
    "        ROOT /\\\n",
    "        'boxmot' /\\\n",
    "        'configs' /\\\n",
    "        (opt.tracking_method + '.yaml')\n",
    "    for i in range(predictor.dataset.bs):\n",
    "        tracker = create_tracker(\n",
    "            predictor.args.tracking_method,\n",
    "            predictor.args.tracking_config,\n",
    "            predictor.args.reid_model,\n",
    "            predictor.device,\n",
    "            predictor.args.half,\n",
    "            predictor.args.per_class\n",
    "        )\n",
    "        predictor.trackers.append(tracker)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run(args):\n",
    "\n",
    "    model = YOLO(args['yolo_model'] if 'v8' in str(args['yolo_model']) else 'yolov8n')\n",
    "    overrides = model.overrides.copy()\n",
    "    model.predictor = TASK_MAP[model.task][3](overrides=overrides, _callbacks=model.callbacks)\n",
    "\n",
    "    # extract task predictor\n",
    "    predictor = model.predictor\n",
    "\n",
    "    # combine default predictor args with custom, preferring custom\n",
    "    combined_args = {**predictor.args.__dict__, **args}\n",
    "    # overwrite default args\n",
    "    predictor.args = IterableSimpleNamespace(**combined_args)\n",
    "    predictor.args.device = select_device(args['device'])\n",
    "    LOGGER.info(args)\n",
    "\n",
    "    # setup source and model\n",
    "    if not predictor.model:\n",
    "        predictor.setup_model(model=model.model, verbose=False)\n",
    "    predictor.setup_source(predictor.args.source)\n",
    "\n",
    "    predictor.args.imgsz = check_imgsz(predictor.args.imgsz, stride=model.model.stride, min_dim=2)  # check image size\n",
    "    predictor.save_dir = increment_path(Path(predictor.args.project) /\n",
    "                                        predictor.args.name, exist_ok=predictor.args.exist_ok)\n",
    "\n",
    "    # Check if save_dir/ label file exists\n",
    "    if predictor.args.save or predictor.args.save_txt:\n",
    "        (predictor.save_dir / 'labels' if predictor.args.save_txt\n",
    "         else predictor.save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Warmup model\n",
    "    if not predictor.done_warmup:\n",
    "        predictor.model.warmup(\n",
    "            imgsz=(1 if predictor.model.pt or predictor.model.triton else predictor.dataset.bs, 3, *predictor.imgsz)\n",
    "        )\n",
    "        predictor.done_warmup = True\n",
    "    predictor.seen, predictor.windows, predictor.batch, predictor.profilers = (\n",
    "        0,\n",
    "        [],\n",
    "        None,\n",
    "        (ops.Profile(), ops.Profile(), ops.Profile(), ops.Profile())\n",
    "    )\n",
    "    predictor.add_callback('on_predict_start', on_predict_start)\n",
    "    predictor.run_callbacks('on_predict_start')\n",
    "\n",
    "    yolo_strategy = get_yolo_inferer(args['yolo_model'])\n",
    "    yolo_strategy = yolo_strategy(\n",
    "        model=model.predictor.model if 'v8' in str(args['yolo_model']) else args['yolo_model'],\n",
    "        device=predictor.device,\n",
    "        args=predictor.args\n",
    "    )\n",
    "    model = Yolo(yolo_strategy)\n",
    "\n",
    "    # Initialize variables to keep track of person time\n",
    "    person_time = {}\n",
    "    person_in_frame = {}\n",
    "\n",
    "    for frame_idx, batch in enumerate(predictor.dataset):\n",
    "        predictor.run_callbacks('on_predict_batch_start')\n",
    "        predictor.batch = batch\n",
    "        path, im0s, vid_cap, s = batch\n",
    "\n",
    "        n = len(im0s)\n",
    "        predictor.results = [None] * n\n",
    "\n",
    "        # Preprocess\n",
    "        with predictor.profilers[0]:\n",
    "            im = predictor.preprocess(im0s)\n",
    "\n",
    "        # Inference\n",
    "        with predictor.profilers[1]:\n",
    "            preds = model.inference(im=im)\n",
    "\n",
    "        # Postprocess moved to MultiYolo\n",
    "        with predictor.profilers[2]:\n",
    "            predictor.results = model.postprocess(path, preds, im, im0s, predictor)\n",
    "        predictor.run_callbacks('on_predict_postprocess_end')\n",
    "\n",
    "        # Visualize, save, write results\n",
    "        n = len(im0s)\n",
    "        for i in range(n):\n",
    "\n",
    "            if predictor.dataset.source_type.tensor:  # skip write, show and plot operations if input is raw tensor\n",
    "                continue\n",
    "            p, im0 = path[i], im0s[i].copy()\n",
    "            p = Path(p)\n",
    "\n",
    "            with predictor.profilers[3]:\n",
    "                # get raw bboxes tensor\n",
    "                dets = predictor.results[i].boxes.data\n",
    "                # get tracker predictions\n",
    "                predictor.tracker_outputs[i] = predictor.trackers[i].update(dets.cpu().detach().numpy(), im0)\n",
    "            predictor.results[i].speed = {\n",
    "                'preprocess': predictor.profilers[0].dt * 1E3 / n,\n",
    "                'inference': predictor.profilers[1].dt * 1E3 / n,\n",
    "                'postprocess': predictor.profilers[2].dt * 1E3 / n,\n",
    "                'tracking': predictor.profilers[3].dt * 1E3 / n\n",
    "            }\n",
    "\n",
    "            # filter boxes masks and pose results by tracking results\n",
    "            model.filter_results(i, predictor)\n",
    "            # overwrite bbox results with tracker predictions\n",
    "            model.overwrite_results(i, im0.shape[:2], predictor)\n",
    "\n",
    "            # write inference results to a file or directory\n",
    "            if (predictor.args.verbose or predictor.args.save or\n",
    "               predictor.args.save_txt or predictor.args.show or\n",
    "               predictor.args.save_id_crops):\n",
    "\n",
    "                s += predictor.write_results(i, predictor.results, (p, im, im0))\n",
    "                predictor.txt_path = Path(predictor.txt_path)\n",
    "\n",
    "                # write MOT specific results\n",
    "                if predictor.args.source.endswith(VID_FORMATS):\n",
    "                    predictor.MOT_txt_path = predictor.txt_path.parent / p.stem\n",
    "                else:\n",
    "                    # append folder name containing current img\n",
    "                    predictor.MOT_txt_path = predictor.txt_path.parent / p.parent.name\n",
    "\n",
    "                if predictor.tracker_outputs[i].size != 0 and predictor.args.save_mot:\n",
    "                    write_MOT_results(\n",
    "                        predictor.MOT_txt_path,\n",
    "                        predictor.results[i],\n",
    "                        frame_idx,\n",
    "                        i,\n",
    "                    )\n",
    "\n",
    "                if predictor.args.save_id_crops:\n",
    "                    for d in predictor.results[i].boxes:\n",
    "                        save_one_box(\n",
    "                            d.xyxy,\n",
    "                            im0.copy(),\n",
    "                            file=increment_path(\n",
    "                                Path(predictor.save_dir) / 'id_crops' / f\"{i:05d}\",\n",
    "                                mkdir=True\n",
    "                            ) / f\"{frame_idx:05d}.jpg\",\n",
    "                            labels=[f\"{predictor.names[int(d.id)]} {d.id:.2f}\"],\n",
    "                            colors=[(d.color if predictor.args.save_rgb else d.color_bw)[[2, 1, 0]]]\n",
    "                        )\n",
    "\n",
    "            # display an image in a window using OpenCV imshow()\n",
    "            if predictor.args.show and predictor.plotted_img is not None:\n",
    "                predictor.show(p.parent)\n",
    "\n",
    "            # save video predictions\n",
    "            if predictor.args.save and predictor.plotted_img is not None:\n",
    "                predictor.save_preds(vid_cap, i, str(predictor.save_dir / p.name))\n",
    "\n",
    "            # Record person time\n",
    "            for d in predictor.results[i].boxes:\n",
    "                if d.cls == 0 and d.id >= 0:  # Person class and valid ID\n",
    "                    person_id = int(d.id)\n",
    "                    if person_id not in person_in_frame:  # Person not encountered before\n",
    "                        person_in_frame[person_id] = frame_idx\n",
    "                    else:  # Person already encountered, update the total time\n",
    "                        person_time[person_id] = frame_idx - person_in_frame[person_id]\n",
    "\n",
    "        predictor.run_callbacks('on_predict_batch_end')\n",
    "\n",
    "    # Print total person time\n",
    "    LOGGER.info(\"Total Person Time:\")\n",
    "    for person_id, time in person_time.items():\n",
    "        frame_rate = vid_cap.get(cv2.CAP_PROP_FPS) if vid_cap.get(cv2.CAP_PROP_FPS) > 0 else 30  # Set default frame rate\n",
    "        LOGGER.info(f\"Person {person_id}: {time} frames ({time / frame_rate:.2f} seconds)\")\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--yolo-model', type=Path, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n",
    "    parser.add_argument('--reid-model', type=Path, default=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt')\n",
    "    parser.add_argument('--tracking-method', type=str, default='deepocsort',\n",
    "                        help='deepocsort, botsort, strongsort, ocsort, bytetrack')\n",
    "    parser.add_argument('--source', type=str, default='0',\n",
    "                        help='file/dir/URL/glob, 0 for webcam')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640],\n",
    "                        help='inference size h,w')\n",
    "    parser.add_argument('--conf', type=float, default=0.5,\n",
    "                        help='confidence threshold')\n",
    "    parser.add_argument('--iou', type=float, default=0.7,\n",
    "                        help='intersection over union (IoU) threshold for NMS')\n",
    "    parser.add_argument('--device', default='',\n",
    "                        help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--show', action='store_true',\n",
    "                        help='display tracking video results')\n",
    "    parser.add_argument('--save', action='store_true',\n",
    "                        help='save video tracking results')\n",
    "    # # class 0 is person, 1 is bycicle, 2 is car... 79 is oven\n",
    "    parser.add_argument('--classes', nargs='+', type=int,\n",
    "                        help='filter by class: --classes 0, or --classes 0 2 3')\n",
    "    parser.add_argument('--project', default=EXAMPLES / 'runs' / 'track',\n",
    "                        help='save results to project/name')\n",
    "    parser.add_argument('--name', default='exp',\n",
    "                        help='save results to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true',\n",
    "                        help='existing project/name ok, do not increment')\n",
    "    parser.add_argument('--half', action='store_true',\n",
    "                        help='use FP16 half-precision inference')\n",
    "    parser.add_argument('--vid-stride', type=int, default=1,\n",
    "                        help='video frame-rate stride')\n",
    "    parser.add_argument('--show-labels', action='store_false',\n",
    "                        help='hide labels when show')\n",
    "    parser.add_argument('--show-conf', action='store_false',\n",
    "                        help='hide confidences when show')\n",
    "    parser.add_argument('--save-txt', action='store_true',\n",
    "                        help='save tracking results in a txt file')\n",
    "    parser.add_argument('--save-id-crops', action='store_true',\n",
    "                        help='save each crop to its respective id folder')\n",
    "    parser.add_argument('--save-mot', action='store_true',\n",
    "                        help='save tracking results in a single txt file')\n",
    "    parser.add_argument('--line-width', default=None, type=int,\n",
    "                        help='The line width of the bounding boxes. If None, it is scaled to the image size.')\n",
    "    parser.add_argument('--per-class', action='store_true',\n",
    "                        help='not mix up classes when tracking')\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    return opt\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    run(vars(opt))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
